"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[554],{18452:(e,t,a)=>{a.r(t),a.d(t,{default:()=>i});let i={src:"/dell-pro-ai-studio//_next/static/media/Dell_Technologies_logo.14d36ae9.svg",height:132,width:1017,blurWidth:0,blurHeight:0}},60545:(e,t,a)=>{a.r(t),a.d(t,{Chip:()=>r});var i=a(95155);a(12115);let r=e=>{let{text:t,onClick:a,variant:r="default"}=e;return(0,i.jsx)("span",{className:"".concat("x:inline-block x:mx-2 x:px-3 x:py-1 x:rounded-full x:text-sm x:font-medium x:cursor-pointer x:transition-colors"," ").concat({default:"x:bg-gray-200 x:text-gray-800 x:hover:bg-gray-300 x:dark:bg-neutral-800 x:dark:text-gray-300 x:dark:hover:bg-neutral-600",primary:"x:bg-blue-100 x:text-blue-800 x:hover:bg-blue-200 x:dark:bg-blue-900/30 x:dark:text-blue-400 x:dark:hover:bg-blue-900/50",success:"x:bg-green-100 x:text-green-800 x:hover:bg-green-200 x:dark:bg-green-900/30 x:dark:text-green-500 x:dark:hover:bg-green-900/50"}[r]),onClick:a,children:t})}},61908:(e,t,a)=>{a.r(t),a.d(t,{default:()=>c});var i=a(95155),r=a(12115);let n=JSON.parse('{"y7":{"L":"https://dellupdater.dell.com/non_du/ClientService/endpointmgmt/dpais/client/Dell.AI.Framework.x64.zip","p":"https://dellupdater.dell.com/non_du/ClientService/endpointmgmt/dpais/client/Dell.AI.Framework.arm64.zip"},"Jn":[{"model":"Clip-vit-base-patch32","license":"MIT license","type":"Embedding","platform":"ARM64","hardware":"NPU","hardwareVendor":"Qualcomm","zipUrl":"https://dellupdater.dell.com/non_du/ClientService/endpointmgmt/DellAIProStudio/Dell.Model.CLIP-vit-base-patch32.NPU.ORT-QNN.arm64.zip","zipName":"Dell.Model.CLIP-vit-base-patch32.NPU.ORT-QNN.arm64.zip","description":"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.","recordSize":"679.51 MB","modelSize":"151M params","supportedLanguages":["English"],"tagline":"Cross-modal model trained for robust image-text pairing.","name":"openai-clip","agentGuid":"E9AB3E38-BFE6-4AD3-8EC5-66905B1798D6","hfRepo":"https://huggingface.co/openai/clip-vit-base-patch32"},{"model":"Clip-vit-base-patch32","license":"MIT license","type":"Embedding","platform":"X64","hardware":"NPU","hardwareVendor":"Intel","zipUrl":"https://dellupdater.dell.com/non_du/ClientService/endpointmgmt/DellAIProStudio/Dell.Model.CLIP-vit-base-patch32.NPU.ORT-OV.x64.zip","zipName":"Dell.Model.CLIP-vit-base-patch32.NPU.ORT-OV.x64.zip","description":"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.","recordSize":"679.51 MB","modelSize":"151M params","supportedLanguages":["English"],"tagline":"ViT-based model for zero-shot image-text similarity tasks.","name":"openai-clip","agentGuid":"E9AB3E38-BFE6-4AD3-8EC5-66905B1798D6","hfRepo":"https://huggingface.co/openai/clip-vit-base-patch32"},{"model":"Devstral-Small-2507","license":"Apache 2.0","type":"LLM","platform":"X64","hardware":"GPU","hardwareVendor":"Nvidia","zipUrl":"http://dellupdater.dell.com/non_du/ClientService/endpointmgmt/dpais/model/llamacpp/Dell.Model.Devstral-Small-2507.GPU.LCPP-CUDA.x64.zip","zipName":"Dell.Model.Devstral-Small-2507.GPU.LCPP-CUDA.x64.zip","description":"Devstral is an open-source agentic LLM for software engineering. It excels at using tools to explore codebases, editing multiple files and power software engineering agents. NOTE: Must install CUDA toolkit v12 to use GPU inference.","recordSize":"12.48 GB","modelSize":"23.6B params","supportedLanguages":["English","French","German","Spanish","Portuguese","Italian","Japanese","Korean","Russian","Chinese","Arabic","Persian","Indonesian","Malay","Nepali","Polish","Romanian","Serbian","Swedish","Turkish","Ukrainian","Vietnamese","Hindi","Bengali"],"tagline":"Devstral is an LLM built for code open-source, powerful, and ready to engineer. NOTE: Must install CUDA toolkit v12 to use GPU inference.","name":"GPU/devstral:24b","agentGuid":"2A4EAFDD-3741-428A-BA93-B17F7C324DC7","hfRepo":"https://huggingface.co/mistralai/Devstral-Small-2507"},{"model":"Granite-4.0-h-small","license":"Apache 2.0","type":"LLM","platform":"X64","hardware":"GPU","hardwareVendor":"Nvidia","zipUrl":"http://dellupdater.dell.com/non_du/ClientService/endpointmgmt/dpais/model/llamacpp/Dell.Model.Granite-4.0-small.GPU.LCPP-CUDA.x64.zip","zipName":"Dell.Model.Granite-4.0-small.GPU.LCPP-CUDA.x64.zip","description":"Granite Small is a 32B parameter instruction-tuned model designed for enterprise-grade AI assistants. It supports multilingual reasoning, summarization, RAG, code generation, and tool-calling with long-context capabilities. NOTE: Must install CUDA toolkit v12 to use GPU inference.","recordSize":"18.1 GB","modelSize":"32.2B params","supportedLanguages":["English","German","Spanish","French","Japanese","Portuguese","Arabic","Czech","Italian","Korean","Dutch","Chinese"],"tagline":"Enterprise ready Granite model for powerful, multilingual AI assistants. NOTE: Must install CUDA toolkit v12 to use GPU inference.","name":"dGPU/granite_4_0_h_small","agentGuid":"2D47BFE0-DF0A-484B-A0C1-64356A27CC23","hfRepo":"https://huggingface.co/ibm-granite/granite-4.0-h-small"},{"model":"Granite-4.0-h-tiny","license":"Apache 2.0","type":"LLM","platform":"X64","hardware":"GPU","hardwareVendor":"Nvidia","zipUrl":"http://dellupdater.dell.com/non_du/ClientService/endpointmgmt/dpais/model/llamacpp/Dell.Model.Granite-4.0-tiny.GPU.LCPP-CUDA.x64.zip","zipName":"Dell.Model.Granite-4.0-tiny.GPU.LCPP-CUDA.x64.zip","description":"Granite Tiny is a 7B parameter MoE instruction-tuned model for multilingual reasoning, summarization, RAG, code, and long-context tasks, optimized for efficient AI assistants and business applications. NOTE: Must install CUDA toolkit v12 to use GPU inference.","recordSize":"4.22 GB","modelSize":"6.67B params","supportedLanguages":["English","German","Spanish","French","Japanese","Portuguese","Arabic","Czech","Italian","Korean","Dutch","Chinese"],"tagline":"Lightweight Granite model for multilingual, long context reasoning and AI assistants. NOTE: Must install CUDA toolkit v12 to use GPU inference.","name":"dGPU/granite_4_0_h_tiny","agentGuid":"D2C55738-847C-4F49-BC0D-813A7A6744E8","hfRepo":"https://huggingface.co/ibm-granite/granite-4.0-tiny-preview"},{"model":"Nomic-embed-text-v1.5","license":"Apache 2.0","type":"Embedding","platform":"ARM64","hardware":"CPU","hardwareVendor":"","zipUrl":"https://dellupdater.dell.com/non_du/ClientService/endpointmgmt/DellAIProStudio/Dell.Model.Nomic-embed-text-v1.5.CPU.ORT.arm64.zip","zipName":"Dell.Model.Nomic-embed-text-v1.5.CPU.ORT.arm64.zip","description":"Nomic-embed-text is a high-performing, open-source text embedding model. It supports a maximum context window of 8,192 tokens, making it well-suited for documents, RAG setups, classification, clustering, and more.","recordSize":"101.14 MB","modelSize":"137M params","supportedLanguages":["English"],"tagline":"Open-source embedder for RAG, clustering, and classification.","name":"nomic-embed-text-v1.5","agentGuid":"B549A8F5-02AF-48FB-93E5-F5CA2802C692","hfRepo":"https://huggingface.co/nomic-ai/nomic-embed-text-v1.5"},{"model":"Nomic-embed-text-v1.5","license":"Apache 2.0","type":"Embedding","platform":"X64","hardware":"CPU","hardwareVendor":"","zipUrl":"https://dellupdater.dell.com/non_du/ClientService/endpointmgmt/DellAIProStudio/Dell.Model.Nomic-embed-text-v1.5.CPU.ORT.x64.zip","zipName":"Dell.Model.Nomic-embed-text-v1.5.CPU.ORT.x64.zip","description":"Nomic-embed-text is a high-performing, open-source text embedding model. It supports a maximum context window of 8,192 tokens, making it well-suited for documents, RAG setups, classification, clustering, and more.","recordSize":"103.64 MB","modelSize":"137M params","supportedLanguages":["English"],"tagline":"Open-source embedder for RAG, clustering, and classification.","name":"nomic-embed-text-v1.5","agentGuid":"B549A8F5-02AF-48FB-93E5-F5CA2802C692","hfRepo":"https://huggingface.co/nomic-ai/nomic-embed-text-v1.5"},{"model":"Phi-3.5-mini-instruct","license":"MIT license","type":"LLM","platform":"ARM64","hardware":"NPU","hardwareVendor":"Qualcomm","zipUrl":"https://dellupdater.dell.com/non_du/ClientService/endpointmgmt/DellAIProStudio/Dell.Model.Phi-3.5-mini-instruct.NPU.Genie.arm64.zip","zipName":"Dell.Model.Phi-3.5-mini-instruct.NPU.Genie.arm64.zip","description":"Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.","recordSize":"1.96 GB","modelSize":"3.8B params","supportedLanguages":["Arabic","Chinese","Dutch","English","French","German","Italian","Russian","Spanish","Ukrainian"],"tagline":"Optimized for ARM64, fast reasoning and long context understanding.","name":"phi3.5","agentGuid":"791B9737-0611-40AB-9627-C6E18FDC10C9","hfRepo":"https://huggingface.co/microsoft/Phi-3.5-mini-instruct"},{"model":"Phi-3.5-mini-instruct","license":"MIT license","type":"LLM","platform":"X64","hardware":"NPU","hardwareVendor":"AMD","zipUrl":"http://dellupdater.dell.com/non_du/ClientService/endpointmgmt/dpais/model/oga-vitis/Dell.Model.Phi-3.5-mini-instruct.NPU.OGA-Vitis.x64.zip","zipName":"Dell.Model.Phi-3.5-mini-instruct.NPU.OGA-Vitis.x64.zip","description":"Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.","recordSize":"2.12 GB","modelSize":"3.8B params","supportedLanguages":["Arabic","Chinese","Dutch","English","French","German","Italian","Russian","Spanish","Ukrainian"],"tagline":"Small, fast, instruction-tuned model with 128K context and strong reasoning.","name":"phi3.5","agentGuid":"791B9737-0611-40AB-9627-C6E18FDC10C9","hfRepo":"https://huggingface.co/microsoft/Phi-3.5-mini-instruct"},{"model":"Phi-3.5-mini-instruct","license":"MIT license","type":"LLM","platform":"X64","hardware":"NPU","hardwareVendor":"Intel","zipUrl":"https://dellupdater.dell.com/non_du/ClientService/endpointmgmt/DellAIProStudio/Dell.Model.Phi-3.5-mini-instruct.NPU.OVGAI.x64.zip","zipName":"Dell.Model.Phi-3.5-mini-instruct.NPU.OVGAI.x64.zip","description":"Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.","recordSize":"1.62 GB","modelSize":"3.8B params","supportedLanguages":["Arabic","Chinese","Dutch","English","French","German","Italian","Russian","Spanish","Ukrainian"],"tagline":"Small, fast, instruction-tuned model with 128K context and strong reasoning.","name":"phi3.5","agentGuid":"791B9737-0611-40AB-9627-C6E18FDC10C9","hfRepo":"https://huggingface.co/microsoft/Phi-3.5-mini-instruct"},{"model":"Phi-4-mini-instruct","license":"MIT license","type":"LLM","platform":"X64","hardware":"NPU","hardwareVendor":"Intel","zipUrl":"https://dellupdater.dell.com/non_du/ClientService/endpointmgmt/dpais/model/ovgai/Dell.Model.Phi-4-mini-instruct.NPU.OVGAI.x64.zip","zipName":"Dell.Model.Phi-4-mini-instruct.NPU.OVGAI.x64.zip","description":"Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures.","recordSize":"1.99 GB","modelSize":"4B params","supportedLanguages":["Arabic","Chinese","Czech","Danish","Dutch","English","Finnish","French","German","Hebrew","Hungarian","Italian","Japanese","Korean","Norwegian","Polish","Portuguese","Russian","Spanish","Swedish","Thai","Turkish","Ukrainian"],"tagline":"A lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data.","name":"phi4","agentGuid":"A9409A4B-833D-4F69-836A-C2EBDC80C9A1","hfRepo":"https://huggingface.co/microsoft/Phi-4-mini-instruct"},{"model":"Whisper-small.en","license":"Apache 2.0","type":"Transcription","platform":"ARM64","hardware":"NPU","hardwareVendor":"Qualcomm","zipUrl":"https://dellupdater.dell.com/non_du/ClientService/endpointmgmt/DellAIProStudio/Dell.Model.Whisper-small.en.NPU.ORT-QNN.arm64.zip","zipName":"Dell.Model.Whisper-small.en.NPU.ORT-QNN.arm64.zip","description":"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model.","recordSize":"1.20 GB","modelSize":"242M params","supportedLanguages":["English"],"tagline":"Accurate ASR model for English with strong out-of-the-box results.","name":"whisper","agentGuid":"9B301E41-0AF7-48EA-8A16-0C6AF3AC8EB3","hfRepo":"https://huggingface.co/openai/whisper-small.en"},{"model":"Whisper-small.en","license":"Apache 2.0","type":"Transcription","platform":"X64","hardware":"NPU","hardwareVendor":"Intel","zipUrl":"https://dellupdater.dell.com/non_du/ClientService/endpointmgmt/DellAIProStudio/Dell.Model.Whisper-small.en.NPU.OVGAI.x64.zip","zipName":"Dell.Model.Whisper-small.en.NPU.OVGAI.x64.zip","description":"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model.","recordSize":"804.61 MB","modelSize":"242M params","supportedLanguages":["English"],"tagline":"Robust English speech-to-text with wide domain generalization.","name":"whisper","agentGuid":"9B301E41-0AF7-48EA-8A16-0C6AF3AC8EB3","hfRepo":"https://huggingface.co/openai/whisper-small.en"}]}');var l=a(72866),o=a(73574),s=a(83416),d=a(92733);function c(){var e;let[t,a]=(0,r.useState)(""),[c,p]=(0,r.useState)("all"),[m,h]=(0,r.useState)("all"),[x,u]=(0,r.useState)("all"),g=((null==(e=n.Jn)?void 0:e.map(e=>{var t;let a=[{label:"Download",href:e.zipUrl,tooltip:e.recordSize},{label:"Model Repository",href:e.hfRepo}];return e.name&&a.push({label:"> dpais model install --model "+e.name}),{title:e.model,platform:e.platform,hardware:e.hardware,hardwareVendor:e.hardwareVendor,type:e.type,text:e.description,tagline:e.tagline,recordSize:e.recordSize,modelSize:e.modelSize,languages:e.supportedLanguages,buttons:a,details:[{icon:"model",text:e.modelSize},{icon:"license",text:null!=(t=e.license)?t:"Apache 2.0"},{icon:"platform",text:"9 Platforms"}]}}))||[]).filter(e=>e.title.toLowerCase().includes(t.toLowerCase())&&("all"===c||e.platform.toLowerCase()===c.toLowerCase())&&("all"===m||e.type===m)&&("all"===x||e.hardware===x)),b=[{label:"x64 Client",href:n.y7.L},{label:"arm64 Client",href:n.y7.p}],f=[{label:"All",onClick:()=>p("all"),isActive:"all"===c},{label:"x64",onClick:()=>p("x64"),isActive:"x64"===c},{label:"arm64",onClick:()=>p("arm64"),isActive:"arm64"===c}];return(0,i.jsxs)("div",{className:"space-y-4",children:[(0,i.jsxs)("h4",{id:"install",className:"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-8 x:text-xl",children:["Install Dell Pro AI Studio Client",(0,i.jsx)("a",{href:"#install-dell-pro-ai-studio-clientinstaller-here",className:"x:focus-visible:nextra-focus subheading-anchor"})]}),(0,i.jsx)("div",{children:(0,i.jsx)(s.ButtonGroup,{buttons:b})}),(0,i.jsx)(d.Input,{type:"text",placeholder:"Search by model...",value:t,onChange:e=>a(e.target.value)}),(0,i.jsx)("div",{className:"px-4",children:(0,i.jsx)(s.ButtonGroup,{buttons:f})}),(0,i.jsx)("div",{className:"px-4",children:(0,i.jsx)(s.ButtonGroup,{buttons:[{label:"All",onClick:()=>h("all"),isActive:"all"===m},{label:"Transcription",onClick:()=>h("Transcription"),isActive:"Transcription"===m},{label:"LLM",onClick:()=>h("LLM"),isActive:"LLM"===m},{label:"Embedding",onClick:()=>h("Embedding"),isActive:"Embedding"===m}]})}),(0,i.jsx)("div",{className:"px-4",children:(0,i.jsx)(s.ButtonGroup,{buttons:[{label:"All",onClick:()=>u("all"),isActive:"all"===x},{label:"CPU",onClick:()=>u("CPU"),isActive:"CPU"===x},{label:"NPU",onClick:()=>u("NPU"),isActive:"NPU"===x},{label:"GPU",onClick:()=>u("GPU"),isActive:"GPU"===x}]})}),(0,i.jsxs)(o.CardGroup,{children:[g.map((e,t)=>(0,i.jsx)(l.Card,{...e},t)),g.length<1&&(0,i.jsx)("p",{className:"mt-4",children:"No results found"})]})]})}},72866:(e,t,a)=>{a.r(t),a.d(t,{Card:()=>p});var i=a(95155),r=a(12115),n=a(6874),l=a.n(n),o=a(60545);let s=e=>{let{className:t}=e;return(0,i.jsxs)("svg",{className:t,width:"16",height:"16",viewBox:"0 0 16 16",fill:"none",xmlns:"http://www.w3.org/2000/svg",children:[(0,i.jsx)("path",{d:"M11 1.5H3.5C2.39543 1.5 1.5 2.39543 1.5 3.5V11",stroke:"currentColor",strokeWidth:"1.5",strokeLinecap:"round",strokeLinejoin:"round"}),(0,i.jsx)("path",{d:"M4.5 4.5H12.5C13.6046 4.5 14.5 5.39543 14.5 6.5V12.5C14.5 13.6046 13.6046 14.5 12.5 14.5H4.5C3.39543 14.5 2.5 13.6046 2.5 12.5V6.5C2.5 5.39543 3.39543 4.5 4.5 4.5Z",stroke:"currentColor",strokeWidth:"1.5",strokeLinecap:"round",strokeLinejoin:"round"})]})},d=e=>{let{className:t}=e;return(0,i.jsx)("svg",{className:t,width:"16",height:"16",viewBox:"0 0 16 16",fill:"none",xmlns:"http://www.w3.org/2000/svg",children:(0,i.jsx)("path",{d:"M13.5 4.5L6.5 11.5L2.5 7.5",stroke:"currentColor",strokeWidth:"1.5",strokeLinecap:"round",strokeLinejoin:"round"})})},c={model:e=>{let{className:t}=e;return(0,i.jsxs)("svg",{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 512 512",width:"1em",height:"1em",preserveAspectRatio:"xMidYMid meet",fill:"currentColor",className:t,children:[(0,i.jsx)("path",{d:"M425.706 142.294A240 240 0 0 0 16 312v88h144v-32H48v-56c0-114.691 93.309-208 208-208s208 93.309 208 208v56H352v32h144v-88a238.432 238.432 0 0 0-70.294-169.706"}),(0,i.jsx)("path",{d:"M80 264h32v32H80zm160-136h32v32h-32zm-104 40h32v32h-32zm264 96h32v32h-32zm-102.778 71.1l69.2-144.173l-28.85-13.848l-69.183 144.135a64.141 64.141 0 1 0 28.833 13.886M256 416a32 32 0 1 1 32-32a32.036 32.036 0 0 1-32 32"})]})},license:e=>{let{className:t}=e;return(0,i.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",fill:"currentColor",focusable:"false",role:"img",width:"1em",height:"1em",preserveAspectRatio:"xMidYMid meet",viewBox:"0 0 12 12",className:t,children:(0,i.jsx)("path",{d:"M2.89453 5.63646V7.2927C2.89453 7.63638 3.17195 7.9138 3.51562 7.9138C3.85929 7.9138 4.13672 7.63638 4.13672 7.2927V5.63646C4.13672 5.29279 3.85929 5.01535 3.51562 5.01535C3.17195 5.01535 2.89453 5.29279 2.89453 5.63646ZM5.37891 5.63646V7.2927C5.37891 7.63638 5.65633 7.9138 6.00001 7.9138C6.34368 7.9138 6.6211 7.63638 6.6211 7.2927V5.63646C6.6211 5.29279 6.34368 5.01535 6.00001 5.01535C5.65633 5.01535 5.37891 5.29279 5.37891 5.63646ZM2.6875 9.98411H9.31251C9.65618 9.98411 9.9336 9.70669 9.9336 9.36302C9.9336 9.01935 9.65618 8.74192 9.31251 8.74192H2.6875C2.34383 8.74192 2.06641 9.01935 2.06641 9.36302C2.06641 9.70669 2.34383 9.98411 2.6875 9.98411ZM7.86329 5.63646V7.2927C7.86329 7.63638 8.14071 7.9138 8.48438 7.9138C8.82805 7.9138 9.10547 7.63638 9.10547 7.2927V5.63646C9.10547 5.29279 8.82805 5.01535 8.48438 5.01535C8.14071 5.01535 7.86329 5.29279 7.86329 5.63646ZM5.61493 1.49169L2.34383 3.21419C2.17406 3.30114 2.06641 3.47919 2.06641 3.66966C2.06641 3.95536 2.29828 4.18724 2.58398 4.18724H9.42016C9.70172 4.18724 9.9336 3.95536 9.9336 3.66966C9.9336 3.47919 9.82594 3.30114 9.65618 3.21419L6.38508 1.49169C6.14493 1.36333 5.85508 1.36333 5.61493 1.49169Z"})})},platform:e=>{let{className:t}=e;return(0,i.jsxs)("svg",{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",role:"img",width:"1em",height:"1em",preserveAspectRatio:"xMidYMid meet",viewBox:"0 0 32 32",fill:"currentColor",className:t,children:[(0,i.jsx)("path",{d:"M11,11V21H21V11Zm8,8H13V13h6Z"}),(0,i.jsx)("path",{d:"M30,13V11H26V8a2,2,0,0,0-2-2H21V2H19V6H13V2H11V6H8A2,2,0,0,0,6,8v3H2v2H6v6H2v2H6v3a2,2,0,0,0,2,2h3v4h2V26h6v4h2V26h3a2,2,0,0,0,2-2V21h4V19H26V13ZM24,24H8V8H24Z"})]})}},p=e=>{let{title:t,text:a,buttons:n,details:p,platform:m=null,type:h,hardware:x,hardwareVendor:u}=e,g=n.filter(e=>!e.label.trim().startsWith(">")),b=n.find(e=>e.label.trim().startsWith(">")),[f,v]=(0,r.useState)(!1),A=async e=>{if(!e)return;let t=e.slice(1).trim();try{await navigator.clipboard.writeText(t),v(!0),setTimeout(()=>v(!1),2e3)}catch(e){console.error("Failed to copy text: ",e)}};return(0,i.jsxs)("div",{className:"x:w-full x:rounded-xl x:border x:border-gray-200 x:bg-white x:dark:bg-neutral-900 x:shadow-sm x:hover:shadow-lg x:transition x:p-4 x:flex x:flex-col x:gap-4",children:[(0,i.jsxs)("h2",{className:"x:text-xl x:font-semibold x:text-gray-900 x:dark:text-gray-100",children:[t,m&&(0,i.jsx)(o.Chip,{text:m.toLowerCase(),onClick:()=>{}}),h&&(0,i.jsx)(o.Chip,{variant:"primary",text:h,onClick:()=>{}}),x&&(0,i.jsx)(o.Chip,{variant:"success",text:"".concat(x," ").concat("CPU"!=x?"- ".concat(u):""),onClick:()=>{}})]}),a&&(0,i.jsx)("p",{className:"x:text-gray-600 x:dark:text-gray-400 x:text-sm",children:a}),p&&p.length>0&&(0,i.jsx)("ul",{className:"x:grid x:grid-flow-col x:auto-cols-max x:items-center x:gap-x-12 x:mt-2",children:p.map((e,t)=>{let a=c[e.icon];return e.tooltip?(0,i.jsxs)("li",{className:"x:relative x:group x:flex x:items-center x:gap-2 x:text-sm",children:[a&&(0,i.jsx)(a,{className:"x:shrink-0"}),(0,i.jsx)("span",{className:"x:whitespace-nowrap",children:e.text}),(0,i.jsxs)("div",{className:"x:absolute x:bottom-full x:left-1/2 x:transform x:-translate-x-1/2 x:mb-2 w-max x:max-w-xs x:px-3 x:py-2 x:rounded-lg x:bg-gray-800 x:text-white x:text-sm x:opacity-0 x:group-hover:opacity-100 x:transition-opacity x:pointer-events-none x:dark:bg-neutral-700 x:whitespace-pre-line x:text-left",role:"tooltip",children:[e.tooltip,(0,i.jsx)("div",{className:"x:absolute x:top-full x:left-1/2 x:transform x:-translate-x-1/2 x:w-0 x:h-0 x:border-x-4 x:border-x-transparent x:border-t-4 x:border-t-gray-800 x:dark:border-t-neutral-700"})]})]},t):(0,i.jsxs)("li",{className:"x:flex x:items-center x:gap-2 x:text-sm",children:[a&&(0,i.jsx)(a,{className:"x:shrink-0"}),(0,i.jsx)("span",{className:"x:whitespace-nowrap",children:e.text})]},t)})}),(0,i.jsxs)("div",{className:"x:flex x:flex-row x:flex-nowrap x:items-center x:justify-between x:gap-4 x:mt-auto x:pt-4",children:[(0,i.jsx)("div",{className:"x:flex x:items-center x:gap-4",children:g.map((e,t)=>(0,i.jsx)(l(),{href:e.href||"#",className:"x:text-sm x:font-medium x:text-gray-700 x:dark:text-gray-300 hover:x:underline",title:e.tooltip,children:e.label},t))}),b&&(0,i.jsx)("div",{className:"x:grow x:min-w-0 x:px-4 x:py-2 x:rounded-lg x:bg-gray-100 x:dark:bg-neutral-800 x:text-gray-800 x:dark:text-gray-200 x:text-sm x:font-mono x:overflow-x-auto",children:(0,i.jsxs)("div",{className:"x:flex x:items-center x:justify-between x:gap-4",children:[(0,i.jsx)("span",{children:b.label}),(0,i.jsx)("button",{onClick:()=>A(b.label),title:f?"Copied!":"Copy code",className:"x:shrink-0 x:p-1 x:rounded-md x:hover:bg-gray-200 x:dark:hover:bg-neutral-700 x:transition-colors",children:f?(0,i.jsx)(d,{className:"x:w-4 x:h-4 x:text-green-500"}):(0,i.jsx)(s,{className:"x:w-4 x:h-4 x:text-gray-500 x:dark:text-gray-400"})})]})})]})]})}},73574:(e,t,a)=>{a.r(t),a.d(t,{CardGroup:()=>r});var i=a(95155);a(12115);let r=e=>{let{children:t}=e;return(0,i.jsx)("div",{className:"x:w-full x:flex x:flex-wrap x:justify-center x:gap-4 x:p-4",children:t})}},83416:(e,t,a)=>{a.r(t),a.d(t,{ButtonGroup:()=>l});var i=a(95155);a(12115);var r=a(6874),n=a.n(r);let l=e=>{let{buttons:t,className:a=""}=e;return(0,i.jsx)("div",{className:"x:inline-flex x:rounded-lg x:shadow-sm x:border x:dark:border-neutral-700 ".concat(a," x:mt-2"),role:"group",children:t.map((e,a)=>{let r=0===a,l=a===t.length-1,o="x:relative x:px-4 x:py-2 x:text-sm x:font-medium x:transition-colors x:focus:z-10 x:focus-visible:nextra-focus";r&&!l?o+=" x:rounded-l-lg":l&&!r?o+=" x:rounded-r-lg":r&&l&&(o+=" x:rounded-lg");let s={};r&&(s.borderTopLeftRadius="0.5rem",s.borderBottomLeftRadius="0.5rem"),l&&(s.borderTopRightRadius="0.5rem",s.borderBottomRightRadius="0.5rem"),l||(s.borderRight="1px solid #797979ff"),r||(o+=" x:-ml-px x:border-l x:border-gray-200 x:dark:border-neutral-700"),l||(o+=" x:border-r x:border-gray-200 x:dark:border-neutral-700"),e.isActive?o+=" active x:bg-gray-100 x:text-white x:dark:bg-black":o+=" x:bg-white x:text-gray-700 x:hover:bg-gray-100 x:dark:bg-neutral-800 x:dark:text-gray-300 x:dark:hover:bg-neutral-700",e.disabled&&(o+=" x:opacity-50 x:cursor-not-allowed");let d=e.href?n():"button";return(0,i.jsx)(d,{href:e.href||void 0,onClick:e.onClick,disabled:e.disabled,className:o,style:s,children:e.label},a)})})}},84546:(e,t,a)=>{a.r(t),a.d(t,{default:()=>i});let i={src:"/dell-pro-ai-studio//_next/static/media/quick-start-cli.b112b86a.png",height:611,width:1172,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAECAMAAACEE47CAAAACVBMVEUTExMeHx0MDAxnKjFWAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAG0lEQVR4nGNgZGRkAANGMICwGRgYmBgYmKAAAAF1ABzB61wvAAAAAElFTkSuQmCC",blurWidth:8,blurHeight:4}},92733:(e,t,a)=>{a.r(t),a.d(t,{Input:()=>r});var i=a(95155);a(12115);let r=e=>{let{id:t,name:a,label:r,type:n="text",placeholder:l,value:o,onChange:s,error:d,helperText:c,disabled:p=!1,className:m=""}=e,h="".concat("x:rounded-lg x:px-3 x:py-4 x:my-4 x:transition-all x:w-full x:md:w-64 x:text-base x:leading-tight x:md:text-sm x:bg-black/[.05] x:dark:bg-gray-50/10 x:placeholder:text-gray-600 x:dark:placeholder:text-gray-400 x:contrast-more:border x:contrast-more:border-current x:[&::-webkit-search-cancel-button]:appearance-none x:transition-colors x:focus-visible:nextra-focus"," ").concat(p?"x:opacity-60 x:cursor-not-allowed x:bg-gray-100 x:dark:bg-neutral-800":d?"x:border-red-500 x:dark:border-red-600":"x:border-gray-300 x:hover:border-gray-400 x:dark:border-neutral-700 x:dark:hover:border-neutral-500");return(0,i.jsxs)("div",{className:"x:w-full ".concat(m),children:[r&&(0,i.jsx)("label",{htmlFor:t,className:" x:text-sm x:font-medium x:mb-1.5 x:text-gray-800 x:dark:text-gray-200",children:r}),(0,i.jsx)("input",{id:t,name:a,type:n,placeholder:l,value:o,onChange:s,disabled:p,className:h,"aria-invalid":!!d,"aria-describedby":d?"".concat(t,"-error"):c?"".concat(t,"-helper"):void 0}),(d||c)&&(0,i.jsx)("p",{id:d?"".concat(t,"-error"):"".concat(t,"-helper"),className:"x:mt-1.5 x:text-sm ".concat(d?"x:text-red-500":"x:text-gray-600 x:dark:text-gray-400"),children:d||c})]})}}}]);